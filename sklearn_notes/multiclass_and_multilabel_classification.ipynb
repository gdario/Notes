{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multiclass and multilabel Classification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "There is some confusion about the difference between multiclass, multilabel, and multioutput classification. Scikit-Learn's documentation about these topics is rigorous, but a bit terse. In what follows, we aim to provide an unambiguous definition of these three types of classificatino, and how one can build classifiers to work on them. We will see that some classifiers are able to perform certain types of classification out of the box, while others need to be *wrapped* in special *meta-estimators* in order to do the same thing.\n",
    "\n",
    "In this article we will first define the three types of classification, we will then show how to generate synthetic datasets for each of the classes. Finally we will see how to build classifiers for each data type, which classifiers need to be modified by wrapping them into a meta-estimator, and which ones do not.\n",
    "\n",
    "**TODO**: find out how to include a TOC in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between multiclass, multilabel and multioutput\n",
    "\n",
    "We start setting some terminology that we will be using throughout the article. A classification problem involves a set of *samples* or *observations* or *instances*. Each sample has a number of **outputs**, and each output is characterized by a number of **classes**. In a binary classification problem, e.g. classifying pictures of cats and dogs, each image has one underlying output (it depicts an animal) and two classes, (cat and dog).\n",
    "\n",
    "### multiclass data\n",
    "\n",
    "The difference between multiclass, multilabel and multioutput data is described in [section 1.12](https://scikit-learn.org/stable/modules/multiclass.html) of the Scikit-Learn User Guide. In a multiclass classification problem we still have only *one output* but *more than two classes*. For example, if our dataset consists of images of cats and dogs, and our goal is to build a classifier for these images, we are dealing with a *binary* classification problem. If, instead, our datasets contains images of cats, dogs and horses, we have a *multiclass* classification problem. A subtle but crucial point here is that we are assuming that each image can contain one and only one animal. An animal in a picture can be one and only one of either a cat, a dog or a horse, but it cannot be a cat and a dog at the same time, nor it cannot contain two cats. In other words, each instance:\n",
    "\n",
    "1. Contains only one entity (a cat, a dogs, or a horse).\n",
    "2. The entities are mutually exclusive.\n",
    "\n",
    "In what follows, wehttps://scikit-learn.org/stable/modules/multiclass.html always assume point 1, but we will relax point 2.\n",
    "\n",
    "If we had 5 images of either animal, we could represent the labels in a few ways.\n",
    "\n",
    "1. An array of labels like `['dog', 'horse', 'cat', 'horse', 'dog']`.\n",
    "2. An array of integers, where `cat`=0, `dog`=1, `horse`=2: The array at point 1, therefore, could be represented as `[1, 2, 0, 2, 1]`.\n",
    "3. A sparse binary matrix of shape `(n_samples, n_classes)`, where columns 0 to 2 are, respectively, `cat`, `dog`, `horse`, and each of them contains a binary variable indicating whether the instance belongs to the class.\n",
    "\n",
    "```\n",
    "[[0, 1, 0],\n",
    " [0, 0, 1],\n",
    " [1, 0, 0],\n",
    " [0, 0, 1],\n",
    " [0, 1, 0]]\n",
    "```\n",
    "\n",
    "This last representation emphasizes the fact that the three classes are mutually exclusive, as each row can only have one entry equal to 1.\n",
    "\n",
    "### multilabel data\n",
    "\n",
    "If we still assume that there is only one output, but we relax the constraint that classes be mutually exclusive, we have a *multilabel* classification problem. A typical example is provided by text documents. Each sample (i.e., document) has one output, its *topic*. The article topic can be classified, for example, into politics, sport, entertainment and crime. Therefore we have a classification problem with one output and four classes. Unlike the multiclass case, however, some articles may contain multiple topics. An article about a corrupted politician facing trial would belong to both the crime *and* politics classes. Similarly an article about an athlete taking part in a charity TV show, would belong to the sport *and* entertainment classes.\n",
    "\n",
    "More formally, we can say that given $K$ classes (4, in our example above), each document can belong to $x$ of them.\n",
    "If an article touches all of politics, sport, entertainment and crime, then $x = K$. What is, however, the smallest number of classes a document can belong to? This depends on how we collect our data. If our dataset also contains articles about computer programming, a subject that does not appear in the four labels above, then we must admit that some articles may not be labeled at all. The lowest possible value of $x$ would, in this case, be 0.\n",
    "If, conversely, we take care of including in our dataset only articles which refer to at least one of the four topics above, then the lowest possible value of $x$ is 1. The Scikit-Learn function `make_multilabel_classification` covers these two scenarios via the `allow_unlabeled` argument, as we will see below.\n",
    "\n",
    "In Scikit-Learn, multilabel datasets must be represented as either sparse or dense matrices of shape `(n_samples, n_classes)`, where each column represents a class. If sample $i$ belongs to class $j, j \\in {0,\\ldots\\, K-1}$, then label $y_{ij} = 1$. Since classes are not mutually exclusive, each row can have multiple, or even all entries equal to 1. If we index our classes as `crime` = 0, `entertainment` = 1, `politics` = 2 and `sport` = 3, a document covering both `politics` and `crime` would be represented as `[1, 0, 1, 0]`, while a document about `entertainment` and `sport` would be represented as `[0, 1, 0, 1]`. A dataset consisting of these two samples would be a dense or sparse matrix of the form\n",
    "\n",
    "```\n",
    "[[1, 0, 1, 0],\n",
    " [0, 1, 0, 1]]\n",
    "```\n",
    "\n",
    "**TODO** include the example about `MultiOutputClassifier`.\n",
    "\n",
    "### multioutput (a.k.a. multitask) classification\n",
    "\n",
    "In a multilabel dataset, each sample can belong to multiple classes, but each individual class is binary. Therefore, an article can be about politics and crime, but when restricting to a given class, say crime, the article is either about crime, or it is not. Put differently, each row of the multilabel array, can only contain either 0 or 1, to indicate the absence or presence of the corresponding label.\n",
    "\n",
    "**TODO** understand whether multioutput generalizes only multiclass or also multilable.\n",
    "\n",
    "1. Each sample has > 1 output.\n",
    "2. Each output has more than 2 classes.\n",
    "\n",
    "Note however, that the classes in each output are mutually exclusive. Scikit-Learn documentation shows an example where, given a set of images, we measure two outputs: the type of fruit and its color. The type of fruit is one of `['apple', 'pear', 'orange']`, and the color is one of `['green', 'red', 'yellow', 'orange']`. These classes are mutually exclusive (we assume, for simplicity, that a fruit is only one color). As such, this would be a generalization of the multiclass case. This example would be represented associating to each output a column vector containing the class label of each instance for that particular output. To clarify, if our dataset contains a red apple, an orange orange and a green pear, we would create two column vectors, one for the fruit and one for the color, and we would combine them as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['apple', 'red'],\n",
       "       ['orange', 'orange'],\n",
       "       ['pear', 'green']], dtype='<U6')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "fruits = ['apple', 'orange', 'pear']\n",
    "colors = ['red', 'orange', 'green']\n",
    "dataset = np.stack([fruits, colors], axis=1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** remember to mention that \" all classifiers handling multioutput-multiclass (also known as multitask classification) tasks, support the multilabel classification task as a special case.\"\n",
    "**NOTE**: it seems that the recommended way to handle multilabel classification problems is via `MultiOututClassifier`. Is this the case?\n",
    "**TODO** mention the difference between methods that treat each each label independently, like `MultiOutputClassifier`, and those that consider them jointly.\n",
    "\n",
    "\n",
    "**TODO** confirm that the case below is not covered.\n",
    "\n",
    "Let's now consider the text document example, with the same four classes as before, but with an additional topic describing the mood of the article. For simplicity let's imagine three classes: `happy`, `sad` and `neutral`. An article about an unjustly convicted athlete who is found innocent after spending years in prison would have a sad beginning and a happy ending. This article would therefore belong both to the `sport` and `crime` classes of the `subject` output and to the `happy` and `sad` classes of the `mood` output. This is a generalization of the multilabel case. If, as before, we index the classes for the `subject` output as `crime` = 0, `entertainment` = 1, `politics` = 2 and `sport` = 3 and the classes for the `mood` output as `happy` = 0, `neutral` = 1, `sad` = 2, the document we just described would be represented as `[1, 0, 0, 1]` for the subject and `[1, 0, 1]` for the mood.\n",
    "\n",
    "At the time of writing, there is no metric in the `sklearn.metrics` module, that supports multioutput-multiclass classification.\n",
    "\n",
    "Summarizing, we can use the following taxonomy (from the Scikit-Learn User Manual) of the models described so far.\n",
    "\n",
    "| Type        | Number of Targets | Target Cardinality | Valid Type of Target     |\n",
    "| ---         | ---               | ---                | ---                      |\n",
    "| Multiclass  | 1                 | > 2                | `multiclass`             |\n",
    "| Multilabel  | > 1               | 2 (0 or 1)         | `multilabel-indicator`   |\n",
    "| Multioutput | > 1               | > 2                | `multiclass-multioutput` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass classification\n",
    "\n",
    "### Fitting a multiclass model\n",
    "\n",
    "let's start creating a synthetic multiclass dataset containing 100 samples each one belonging to one of three possible classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 2, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "x1, y1 = make_classification(n_samples=100, n_features=20, n_informative=5,\n",
    "                             n_classes=3, random_state=42)\n",
    "y1[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split this dataset into a training and test set comprising 80% and 20% of the dataset, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80, 20), (80,), (20, 20), (20,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, test_size=0.2, stratify=y1,\n",
    "                                                       random_state=42)\n",
    "x1_train.shape, y1_train.shape, x1_test.shape, y1_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LogisticRegression` has a `multi_class` argument equal to `auto` by default. This estimator is described as \"inherently multiclass\". Let's fit this estimator with default settings. As we can see, the model infers from the format of the targets that this is a multiclass problem, and return predictions in the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 2, 0, 0, 1]), array([0, 1, 2, 0, 1]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(x1_train, y1_train)\n",
    "yhat1_test = clf.predict(x1_test)\n",
    "y1_test[:5], yhat1_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can be more explicit and set `multi_class='multinomial'`. The result is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(multi_class='multinomial')\n",
    "clf.fit(x1_train, y1_train)\n",
    "tmp = clf.predict(x1_test)\n",
    "np.all(tmp == yhat1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing the performance of a multiclass classifier\n",
    "\n",
    "The accuracy of a multiclass classifier is assessed the same way as a binary classifier: we simply count the fraction of samples where our prediction is identical to the actual target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy_score(y1_test, yhat1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(y1_test == yhat1_test) / len(y1_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, scores like precision and recall, that rely on the idea of \"positive\" and \"negative\", are less obvious. We must therefore provide information on how such terms should be interpreted. If we write `f1_score(y1_test, yhat1_test)` we get an error, which tells us that\n",
    "\n",
    "```\n",
    "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']\n",
    "```\n",
    "\n",
    "In other words, if we consider each of the 3 classes in turn as the \"positives\" and the other classes as the \"negatives\", we end up with three precision scores. The question is how we should average these three scores. The possibilities are `None`, `micro`, `macro`, `weighted`.\n",
    "\n",
    "The first option, `None`, does not average at all, but rather returns the precision values for each class when considered as the positive one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.83333333, 0.76923077, 0.66666667])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_none = f1_score(y1_test, yhat1_test, average=None)\n",
    "f1_none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`macro` computes the precision of each of the three classes without taking the class cardinality into consideration, i.e., classes with a very large number of samples have the same weight in the average (in our case 1/3) than classes with very few samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7564102564102564"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_macro = f1_score(y1_test, yhat1_test, average='macro')\n",
    "f1_macro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the definition above, the value of `f1_macro` should simply be the average of the values of `f1_none`. Let's verify that this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7564102564102564"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_none.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how the `micro` average works, let's consider a simpler example: we have 3 classes and two samples per class. We compare the true labels with our predictions, `yhat_simple`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_simple    = np.array([0, 1, 0, 2, 1, 2])\n",
    "yhat_simple = np.array([0, 1, 0, 0, 2, 2])\n",
    "\n",
    "accuracy_score(y_simple, yhat_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We correctly classify the two samples in the 0 class, we also erroneously misclassify a sample in class 2 as a 0. Therefore precision for class 0 is 2/3.\n",
    "For class 1 we correctly classify one sample and miss the other. Precision for class 1 is 1.\n",
    "For class 2 we have the one correct and one incorrect prediction, the precision for this class is therefore 1/2. We also misclassify a 1 for a 2. Therefore, if $p_k$ represents precision for class $k, k \\in {0, 1, 2}$, we have:\n",
    "\n",
    "$p_0 = 2/3$, $p_1 = 1$ and $p_2 = 1/2$.\n",
    "\n",
    "If we average the three precisions we get the `macro` estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7222222222222222"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2/3 + 1 + 1/2)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7222222222222222"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_simple, yhat_simple, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** re-read this paragraph and make sure it is correct.\n",
    "\n",
    "In the `micro` case we consider each class and its support separately and we add them together. More precisely:\n",
    "\n",
    "For class 0 we have 3 predictions, 2 of which are correct. Support for class 0 is $n_0 = 3$ and $p_0 = 2/3$.\n",
    "For class 1 we have 1 prediction, which is correct. Support $n_1$ is 1 and $p_1 = 1$.\n",
    "For class 2 we have 2 predictions, one of which is correct. Support $n_2 = 2$, $p_2 = 1/2$.\n",
    "\n",
    "$$\n",
    "\\frac{n_0 p_0 + n_1 p_1 + n_2 p_2}{n_0 + n_1 + n_2} = \n",
    "\\frac{3 * 2/3 + 1 * 1 + 2 * 1/2}{3 + 1 + 2} = \\frac{4}{6} = 0.66666\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_simple, yhat_simple, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weighted case computes the per-class metric and uses the class cardinality as a weight to compute the average. Since in our case all classes have size 2, this ends up being the same as the macro average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7222222222222222"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_simple, yhat_simple, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`classification_report` prints this information, and more, in tabular form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80         2\n",
      "           1       1.00      0.50      0.67         2\n",
      "           2       0.50      0.50      0.50         2\n",
      "\n",
      "    accuracy                           0.67         6\n",
      "   macro avg       0.72      0.67      0.66         6\n",
      "weighted avg       0.72      0.67      0.66         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_simple, yhat_simple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this same metric for our synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.71      0.83         7\n",
      "           1       0.71      0.83      0.77         6\n",
      "           2       0.62      0.71      0.67         7\n",
      "\n",
      "    accuracy                           0.75        20\n",
      "   macro avg       0.78      0.75      0.76        20\n",
      "weighted avg       0.78      0.75      0.76        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y1_test, yhat1_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use micro, macro, weighted etc.\n",
    "\n",
    "According to the Scikit-Learn User Manual (section 3.3.2.1):\n",
    "\n",
    "-  In problems where infrequent classes are nonetheless important, `macro`-averaging may be a means of highlighting their performance. On the other hand, the assumption that all classes are equally important is often untrue, such that macro-averaging will over-emphasize the typically low performance on an infrequent class.\n",
    "- `micro` averaging may be preferred in multilabel settings, including multiclass classification where a majority class is to be ignored.\n",
    "- `weighted` averaging accounts for class imbalance by computing the average of binary metrics in which each class’s score is weighted by its presence in the true data sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilabel classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a multilabel dataset with the `make_multilabel_classification` function from `datasets`. If we use the function without specifying any argument, we will get 100 samples and 5 binary classes, where each sample can belong to multiple classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0],\n",
       "       [1, 1, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_multilabel_classification\n",
    "\n",
    "x2, y2 = make_multilabel_classification(random_state=42)\n",
    "y2[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the same targets as a list by setting the `return_indicator` argument to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3], [0, 1, 2], [2, 3], [0], [0, 2]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, y2 = make_multilabel_classification(return_indicator=False, random_state=42)\n",
    "y2[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a `n_labels` argument, but when we set it to, say, 3, we don't get the expected output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we have 100 samples and 5 classes. Each sample belongs to a number of classes $x$, where $x$ is sampled from a Poisson distribution centered around `n_labels`. If we allow some samples to be unlabeled, by setting the `allow_unlabeled` option to `True`, we set the lower bound of `x` to zero, i.e., a sample can belong to zero classes but not to a negative number of classes. If `allow_unlabeled=False`, the lower bound on `x` is 1, i.e., each sample must belong to at least one class. The maximum number of classes a sample can belong to is obviously the total number of available classes. `n_labels` determine the distribution of the number of classes each sample can belong to. If we set `n_labels=1` and we allow for unlabeled samples, most samples will be unlabeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, y2 = make_multilabel_classification(n_labels=1, random_state=42, allow_unlabeled=True)\n",
    "y2[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we impose that all samples must belong to at least one class, and `n_labels=1`, most samples will belong to only one class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, y2 = make_multilabel_classification(n_labels=1, random_state=42, allow_unlabeled=False)\n",
    "y2[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, if we set `n_labels=5` samples will tend to belong to many classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 0],\n",
       "       [0, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 0, 1],\n",
       "       [1, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, y2 = make_multilabel_classification(n_labels=5, random_state=42, allow_unlabeled=False)\n",
    "y2[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dataset of 100 samples with 20 features, 4 classes where on average each sample can belong to 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0]\n",
      " [1 0 1 0]\n",
      " [0 1 0 1]\n",
      " [0 0 1 0]\n",
      " [0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "x2, y2 = make_multilabel_classification(n_samples=100, n_features=20, n_labels=2, n_classes=4,\n",
    "                                       random_state=42)\n",
    "print(y2[:5])\n",
    "x2_train, x2_test, y2_train, y2_test = train_test_split(x2, y2, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is not listed among the functions that support multilabel classification. If we create a `LogisticRegression` estimator and we fit it to `x2` and `y2`, we get an error:\n",
    "\n",
    "> `y should be a 1d array, got an array of shape (80, 4) instead.`\n",
    "\n",
    "We will try two approaches:\n",
    "\n",
    "1. Wrapping the estimator into `OneVsRestClassifier()`.\n",
    "2. Wrapping the estimator into `MultiOutputClassifier()`.\n",
    "\n",
    "### `OneVsRestClassiffier()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "clf_ovr = OneVsRestClassifier(LogisticRegression())\n",
    "clf_ovr.fit(x2_train, y2_train)\n",
    "yhat2_test_ovr = clf_ovr.predict(x2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 1],\n",
       "       [1, 1, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat2_test_ovr[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `MultiOutputClassifier()`\n",
    "\n",
    "If we wrap `LogisticRegression()` into `MultiOutputClassifier()` we obtain the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "clf_moc = MultiOutputClassifier(LogisticRegression())\n",
    "clf_moc.fit(x2_train, y2_train)\n",
    "yhat2_test_moc = clf_moc.predict(x2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 1]\n",
      " [1 1 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(yhat2_test_moc[:5])\n",
    "print(np.all(yhat2_test_moc == yhat2_test_ovr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One difference between these two approaches is the representation of the classification probabilities. The model wrapped into `OneVsRest` returns an array, while the model wrapped into `MultiOutputClassifier()` returns a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, list)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob2_test_ovr = clf_ovr.predict_proba(x2_test)\n",
    "prob2_test_moc = clf_moc.predict_proba(x2_test)\n",
    "\n",
    "type(prob2_test_ovr), type(prob2_test_moc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $p_{ij}$ element of the NumPy array representation contains the probability for sample $i$ of belonging to class $j$. Looking at the first row of the array below, we see that the probabilities of belonging to class 0 and 2 are above 0.9, while class 3 is barely above 0.5, but this is enough to predict that the sample belongs also to this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92785382, 0.01756551, 0.97444888, 0.51609456],\n",
       "       [0.996253  , 0.92493145, 0.00850985, 0.67339801],\n",
       "       [0.00191349, 0.25360644, 0.01994427, 0.9999983 ],\n",
       "       [0.0628209 , 0.00164815, 0.04906532, 0.00534206],\n",
       "       [0.00197308, 0.42121543, 0.24990504, 0.00276496]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob2_test_ovr[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list representation contains as many elements as we have classes (4 in our case). If we look into the first element, we see that:\n",
    "\n",
    "1. The sum of the elements in each row is 1.\n",
    "2. The values in the second column are identical to the values in the first column of the array representation.\n",
    "\n",
    "The interpretation is that the $k$th element of this list contains, for each sample, the probabilities of being a negative (column 0) or a positive (column 1) for class $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.07214618, 0.92785382],\n",
       "       [0.003747  , 0.996253  ],\n",
       "       [0.99808651, 0.00191349],\n",
       "       [0.9371791 , 0.0628209 ],\n",
       "       [0.99802692, 0.00197308]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(prob2_test_moc))\n",
    "prob2_test_moc[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics in the multilabel case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation of a multilabel model adds one additional layer of complexity, in that we must decide how to handle the various labels. Samples will usually belong to different numbers of classes. Suppose we have the targets and the predictions shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgts = [[0, 0, 0, 1],\n",
    "        [1, 1, 1, 1]]\n",
    "preds = [[1, 1, 1, 1],\n",
    "         [1, 1, 1, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first case we are correctly predicting the positive class, but misclassifying all the others. In the second case we are misclassifying one class, but correctly predicting the other 3. Is the first one correctly classified or not? Shall we somehow weight for the fact that the first prediction is 3/4 wrong and the second is 3/4 right?\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "If we use the `accuracy_score` without further arguments, we get a value of zero. This happens because the default behavior is that if the prediction is not identical to the target, the whole sample is incorrect. Therefore we have two misclassification over two samples, and an accuracy of 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(tgts, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, recall, f1 score\n",
    "\n",
    "For multilabel problems, the notion of precision and recall can be applied to each label independently. As before, the question becomes how to average the class-specific values. Let's start with `average=None` where we compute the per-class precision score. Precision is defined as the fraction of positives out of what we classify as positive. For class 0 we predict that both labels are positive, but only one is, therefore $p_0 = 0.5$. Same for $p_1$ and $p_2$. The last label is different, in that we predict one positive, which is an actual positive, and one negative. Only positives are considered in the computation of precision, therefore $p_3 = 1$. Let's verify this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5, 0.5, 1. ])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_none = precision_score(tgts, preds, average=None)\n",
    "p_none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `macro` average of precision is the average of the entries of `p_none`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(tgts, preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_none.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `micro` version is given by observing that:\n",
    "\n",
    "1. We predict 2 positives, with one correct for classes 0, 1, and 2. For each of these 3 classes $p = 0.5$ and support = 2/\n",
    "2. We predict 1 positive for class 3, and it is correct. $p = 1$ and support = 1.\n",
    "\n",
    "Therefore, $p_0 = p_1 = p_2 = 0.5$, $p_3 = 1$ and $n_0 = n_1 = n_2 = 2$ and $n_3 = 1$. Therefore:\n",
    "\n",
    "$$ \\frac{n_0 p_0 + n_1 p_1 + n_2 p_2 + n_3 p_3}{n_0 + n_1 + n_2 + n_3} = \\frac{4}{7} = 0.5714285714285714$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2*0.5 + 2*0.5 + 2*0.5 + 1) / 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(tgts, preds, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO Log-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
