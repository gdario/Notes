{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the Fashion MNIST dataset into a TFRecords object\n",
    "\n",
    "In this notebook we build a simple TFRecords object from (almost) scratch. Our starting point is the Fashion MNIST dataset in Numpy form as available in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.datasets import fashion_mnist\n",
    "%matplotlib inline\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the numpy arrays representing the images and the lables from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 10000\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "print(x_train.shape[0], x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are already stored as Numpy 28 x 28 arrays with dtype uint8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "<class 'numpy.ndarray'>\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0].shape)\n",
    "print(type(x_train))\n",
    "print(x_train.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a validation set\n",
    "\n",
    "We create three data splits, for the training, validation and test sets. For both the validation and the test sets we consider 10000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28) (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "x_val, y_val = x_train[-10000:], y_train[-10000:]\n",
    "x_train, y_train = x_train[:-10000], y_train[:-10000]\n",
    "print(x_train.shape, x_val.shape)\n",
    "\n",
    "data_splits = {'train': (x_train, y_train), 'val': (x_val, y_val), 'test': (x_test, y_test)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of the TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_split in data_splits.keys():\n",
    "    x, y = data_splits[data_split]\n",
    "    output_file = os.path.join('../data/fashion_mnist', data_split + '.tfrecords')\n",
    "    writer = tf.python_io.TFRecordWriter(output_file)\n",
    "    for index in range(x.shape[0]):\n",
    "        image = x[index]\n",
    "        image_raw = image.tostring()\n",
    "        example = tf.train.Example(\n",
    "            features=tf.train.Features(\n",
    "                feature={                    \n",
    "                    'height': tf.train.Feature(int64_list=tf.train.Int64List(\n",
    "                        value=[image.shape[0]])),\n",
    "                    'width': tf.train.Feature(int64_list=tf.train.Int64List(\n",
    "                        value=[image.shape[1]])),\n",
    "                    'label': tf.train.Feature(int64_list=tf.train.Int64List(\n",
    "                        value=[y[index]])),\n",
    "                    'image_raw': tf.train.Feature(bytes_list=tf.train.BytesList(\n",
    "                        value=[image_raw]))   \n",
    "                }))\n",
    "        writer.write(example.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The commands above have created three TFRecords objects, one for each data split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 123264\r\n",
      "-rw-r--r--  1 gdario  staff   8.3M Mar 19 21:05 test.tfrecords\r\n",
      "-rw-r--r--  1 gdario  staff    42M Mar 19 21:04 train.tfrecords\r\n",
      "-rw-r--r--  1 gdario  staff   8.3M Mar 19 21:05 val.tfrecords\r\n"
     ]
    }
   ],
   "source": [
    "%ls -lh ../data/fashion_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFRecord iterators\n",
    "\n",
    "To read back the images and the labels from these serialized strings, we can use different approaches. The first one is to use TFRecord iterators defined in `tf.python_io.tf_record_iterator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_iterator = tf.python_io.tf_record_iterator('../data/fashion_mnist/val.tfrecords')\n",
    "serialized_img = next(record_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reconstruct the original image we need to create an instance of the `Example` class, and convert the serialized string into a numpy array. **Note** that `fromstring` is now deprecated in favor of `frombuffer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = tf.train.Example()\n",
    "example.ParseFromString(serialized_img)\n",
    "\n",
    "image = example.features.feature['image_raw'].bytes_list.value\n",
    "label = example.features.feature['label'].int64_list.value[0]\n",
    "height = example.features.feature['height'].int64_list.value[0]\n",
    "width = example.features.feature['width'].int64_list.value[0]\n",
    "\n",
    "img_flat = np.frombuffer(image[0], dtype=np.uint8)\n",
    "img_reshaped = img_flat.reshape((height, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvFvnyVgAAEJlJREFUeJzt3X+MVeWdx/HP1xGUXwpIGMGCdBHNGoOwGfzVZsP6g+imAjXBlMTIZjfSP6rZxk1c5Z+abJo0m2139w/TCIEUkta2CVpJs7FtzEa7yUZEYyqFbUsIFpaBEUftAAoO890/5rAZcc7z3Lnn3nsuft+vhMzM/d5zz3PP3A/n3nnO8zzm7gIQzyV1NwBAPQg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgLu3kzsyMywmBNnN3a+R+lc78Znavmf3OzA6Y2ZNVHgtAZ1mz1/abWY+k30u6R9IRSa9LWu/u+xLbcOYH2qwTZ/5bJB1w94PuflbSjyWtqfB4ADqoSvivkXR4zM9Hits+xcw2mtkeM9tTYV8AWqzKH/zGe2vxmbf17r5Z0maJt/1AN6ly5j8iacGYn78g6Wi15gDolCrhf13SEjP7oplNlvQ1Sbta0ywA7db02353HzazRyX9QlKPpG3u/tuWtQxAWzXd1dfUzvjMD7RdRy7yAXDxIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gqI4u0Y3xmaUnW+3kDMsXuuSS9PlhZGQkWU89t9zzmjlzZrK+evXqZH3Hjh3JekrueVf9naS279TrgTM/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRVaZVeMzskaUjSOUnD7t6XuT+r9DahSr9vbtucqn3KVfr5H3nkkWT94YcfTtY//vjj0tr999/f9LatUOW45DS6Sm8rLvL5K3c/0YLHAdBBvO0Hgqoafpf0SzN7w8w2tqJBADqj6tv+L7n7UTObK+lXZvY/7v7q2DsU/ynwHwPQZSqd+d39aPF1QNILkm4Z5z6b3b0v98dAAJ3VdPjNbJqZzTj/vaRVkva2qmEA2qvK2/5eSS8UXRaXSvqRu7/UklYBaLtK/fwT3hn9/E3p5n7+do493759e7I+efLkpve9e/fu5LZbtmxJ1k+fPt30vnNyz+vs2bO5fTf0S6erDwiK8ANBEX4gKMIPBEX4gaAIPxAUXX1doJun7s7p6elJ1s+dO1dae+qpp5LbPvTQQ8n60aNHk/Xh4eGmao04duxYsp4bEjw4OFhaW7p0aXLb9evXl9bOnDmjkZERuvoAlCP8QFCEHwiK8ANBEX4gKMIPBEX4gaBYorsL1DlsNvfYuXpuie4q23744YfJeq6vPrXM9kcffZTc9sorr0zWZ82alaznnltvb29p7fDhw8ltU0N6J/Ja4MwPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0HRz38RaOd4/jr7+W+77bZkferUqcn6wMBAsr548eLSWm7661OnTiXrue1T8xjk5I5plWM+Fmd+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwgq289vZtskfUXSgLvfVNw2W9JPJC2SdEjSg+7+fvuaibpU6a+WpPvuu6+0tnbt2uS2r7zySrK+b9++ZD11fcTll1+e3HbGjBnJem5e/kmTJiXrqSW+Fy5cmNy2VRo58/9A0r0X3PakpJfdfYmkl4ufAVxEsuF391clXbi8yBpJ24vvt0tK/xcOoOs0+5m/1937Jan4Ord1TQLQCW2/tt/MNkra2O79AJiYZs/8x81sniQVX0tHWLj7Znfvc/e+JvcFoA2aDf8uSRuK7zdIerE1zQHQKdnwm9lzkv5b0g1mdsTM/k7SdyTdY2Z/kHRP8TOAi4h1cu13M+veheYvYqn56XO/36q//1WrViXrmzZtKq3lxuMfOHAgWb/11luT9Z6entLa+++nL0sZHLywg+vTcsctN+9/6rnn+vnXrVtXWjtz5oxGRkbSkzAUuMIPCIrwA0ERfiAowg8ERfiBoAg/ENTnZurudi5j3W65tqe68qT0VM65552a3lqSVq5cmaw/8MADyXpqKey9e/cmt12+fHmynhuWO23atKZqkjR3bnq4Sm6J76GhoWR9ypQppbXc85o8eXJpLTel+Fic+YGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gqI738+f6tFNSfda5/uwq+62q6r6rTJ+dG3L7xBNPNP3YkvTSSy8l66mhrStWrEhu29vbm6yfPHkyWf/ggw9Ka5dddlly29zrKbfv3NTeqWszpk+fntx2zpw5Te93LM78QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxBUx/v5q4yrT03FXHW/qX7XqqrOJZB73nfeeWdp7bHHHktuu3PnzmR9wYIFyXpuPoDrrruutDY8PJzc9tixY8n6mTNnkvXU7zQ1Jr4RVZf4TsnNNTB79uzSWu6YjcWZHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCyvbzm9k2SV+RNODuNxW3PS3pEUnvFnfb5O7/0a5GnldlXHtVqTH5U6dOTW6b6xNesmRJsn777bcn67NmzSqt7d69O7nt1VdfnazPnz8/Wb/iiiuS9dT89rm+9tyY+9TzltLrHeTmWMjNf5/bvso6Ep2ae6KRM/8PJN07zu3/6u7Lin9tDz6A1sqG391flTTYgbYA6KAqn/kfNbPfmNk2M0u//wLQdZoN//clLZa0TFK/pO+W3dHMNprZHjPb0+S+ALRBU+F39+Pufs7dRyRtkXRL4r6b3b3P3fuabSSA1msq/GY2b8yPX5WUXm4VQNdppKvvOUkrJc0xsyOSviVppZktk+SSDkn6ehvbCKANsuF39/Xj3Ly1mZ2ZWbJvNzc+OzW2/I477sjuOyXXZ5wae56bCyC1FruU7o+W8tc3pPrD582bV1qT8mPHc9coTJo0KVn/5JNPSmu545abxyA3H0BK1fkbcs/79OnTyXqVuSlScwVM5HG5wg8IivADQRF+ICjCDwRF+IGgCD8QVEen7nb3ZHfeVVddldz+8ccfL63lpizOdQvlugJTXVa5rrrc0NVcV16VrsKJLNnczPa5rqVU23PHrZ3dcbljeuml6Wjkjkvu9ZZqW+6xU0t059o9Fmd+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiq40t0p6xevTpZT10HkOvfzC2TnZuqOdUXn+szzl1DkBsemts+1V+e64fP7Tu3fa5eZRrqXNtyfemp31lqSnEpP5Q5dd2HVO06gtxrMTWEeyLHmzM/EBThB4Ii/EBQhB8IivADQRF+ICjCDwTV0X7+KVOm6IYbbiitX3vttcntDx8+XFpLTestpac7lqpdJzA0NJTcNjdeP9dnnOu7TfX75p531eWgc/3hqesAcse86rThqX3njnluSvOqS3yn5J5XlaXHP/U4Dd8TwOcK4QeCIvxAUIQfCIrwA0ERfiAowg8Ele3nN7MFknZIulrSiKTN7v7vZjZb0k8kLZJ0SNKD7v5+6rGGh4d14sSJ0nqVsecDAwPJbQcHB5P13BLdU6dOLa3l1hvI9Rnn5hrISR23Kv3wUn7u/NyY+irPLdcXX2Xu/FOnTjW9rZRfgjt37UfquaVea5I0e/bs0lqrl+gelvQP7v7nkm6T9A0zu1HSk5Jedvclkl4ufgZwkciG39373f3N4vshSfslXSNpjaTtxd22S1rbrkYCaL0JfeY3s0WSlkt6TVKvu/dLo/9BSJrb6sYBaJ+Gr+03s+mSdkr6prv/qdFriM1so6SN0sQ+jwBor4bO/GY2SaPB/6G7P1/cfNzM5hX1eZLG/Yubu2929z5378stzAigc7JptNFT/FZJ+939e2NKuyRtKL7fIOnF1jcPQLtYrivGzL4s6deS3tZoV58kbdLo5/6fSloo6Y+S1rl7sj/NzCr1aS1btqy0dvfddye3vfHGG5P13DDKkydPltZyS3BXWca6ke2ryHVD5t6t5bpI33333dJarrvsvffeS9ZTXV45uW7C3Ouh6tTeqf3ntj148GBpbevWrerv72/oM3n2M7+7/5eksge7q5GdAOg+fAgHgiL8QFCEHwiK8ANBEX4gKMIPBNXxJbpTfda5Ka7feuutpmpSvi9+6dKlyfqKFStKa9dff31y2/nz5yfrueGjM2fOTNZTw25z/fi5odDPPvtssv7OO+8k66mh1Ll+/Nw1KHfdle5pfuaZZ0pruddabshvrp8/NxQ6dY1Dbtrv48ePl9ZaPaQXwOcQ4QeCIvxAUIQfCIrwA0ERfiAowg8ElR3P39KdZcbzV5npJ9evGlVuXHo7p97OyU0FV3XfN998c2ktN4dCbrx/rp7rb08d99zvJDWe393l7g2N5+fMDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBdVU/P4Dq6OcHkET4gaAIPxAU4QeCIvxAUIQfCIrwA0Flw29mC8zsP81sv5n91sz+vrj9aTP7XzN7q/j31+1vLoBWyV7kY2bzJM1z9zfNbIakNyStlfSgpJPu/i8N74yLfIC2a/Qin+yKPe7eL6m/+H7IzPZLuqZa8wDUbUKf+c1skaTlkl4rbnrUzH5jZtvMbFbJNhvNbI+Z7anUUgAt1fC1/WY2XdIrkr7t7s+bWa+kE5Jc0j9p9KPB32Yeg7f9QJs1+ra/ofCb2SRJP5f0C3f/3jj1RZJ+7u43ZR6H8ANt1rKBPTY6xepWSfvHBr/4Q+B5X5W0d6KNBFCfRv7a/2VJv5b0tqTz82NvkrRe0jKNvu0/JOnrxR8HU4/FmR9os5a+7W8Vwg+0H+P5ASQRfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgspO4NliJyS9M+bnOcVt3ahb29at7ZJoW7Na2bZrG71jR8fzf2bnZnvcva+2BiR0a9u6tV0SbWtWXW3jbT8QFOEHgqo7/Jtr3n9Kt7atW9sl0bZm1dK2Wj/zA6hP3Wd+ADWpJfxmdq+Z/c7MDpjZk3W0oYyZHTKzt4uVh2tdYqxYBm3AzPaOuW22mf3KzP5QfB13mbSa2tYVKzcnVpau9dh124rXHX/bb2Y9kn4v6R5JRyS9Lmm9u+/raENKmNkhSX3uXnufsJn9paSTknacXw3JzP5Z0qC7f6f4j3OWu/9jl7TtaU1w5eY2ta1sZem/UY3HrpUrXrdCHWf+WyQdcPeD7n5W0o8lramhHV3P3V+VNHjBzWskbS++367RF0/HlbStK7h7v7u/WXw/JOn8ytK1HrtEu2pRR/ivkXR4zM9H1F1LfrukX5rZG2a2se7GjKP3/MpIxde5NbfnQtmVmzvpgpWlu+bYNbPidavVEf7xVhPppi6HL7n7X0i6T9I3ire3aMz3JS3W6DJu/ZK+W2djipWld0r6prv/qc62jDVOu2o5bnWE/4ikBWN+/oKkozW0Y1zufrT4OiDpBY1+TOkmx88vklp8Hai5Pf/P3Y+7+zl3H5G0RTUeu2Jl6Z2Sfujuzxc3137sxmtXXcetjvC/LmmJmX3RzCZL+pqkXTW04zPMbFrxhxiZ2TRJq9R9qw/vkrSh+H6DpBdrbMundMvKzWUrS6vmY9dtK17XcpFP0ZXxb5J6JG1z9293vBHjMLM/0+jZXhod8fijOttmZs9JWqnRUV/HJX1L0s8k/VTSQkl/lLTO3Tv+h7eStq3UBFdublPbylaWfk01HrtWrnjdkvZwhR8QE1f4AUERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8I6v8AMR+PG8WG7DQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(img_reshaped.shape)\n",
    "plt.imshow(img_reshaped, cmap='gray')\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Data API\n",
    "\n",
    "In this section we examine the `tf.data.Dataset` API. The pipeline is supposed to be flexible, allowing to pull images from different TFRecords, applying random transformations to them, and collecting the transformed images into batches.\n",
    "In our case, we want to create `Dataset`s starting from TFRecords. We will use the `TFRecordDataset` class for this. Let's start creating a dataset for the training set and one for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "training_set = tf.data.TFRecordDataset('../data/fashion_mnist/train.tfrecords')\n",
    "validation_set = tf.data.TFRecordDataset('../data/fashion_mnist/val.tfrecords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unce we have a dataset, we can iterate on it with a dataset iterator. There are several types of iterators, the simplest of which is the *one-shot iterator*. In the example below we extract the first element from the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'IteratorGetNext:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_shot_iterator = validation_set.make_one_shot_iterator()\n",
    "next_element = one_shot_iterator.get_next()\n",
    "next_element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the element contained in `next_element` is a tensor of dtype `string`. To visualize the string, we need to evaluate the tensor in a session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\n\\xd8\\x06\\n\\x0f\\n\\x06height\\x12\\x05\\x1a\\x03\\n\\x01\\x1c\\n\\x0e\\n\\x05label\\x12\\x05\\x1a\\x03\\n\\x01\\t\\n\\xa4\\x06\\n\\timage_raw\\x12\\x96\\x06\\n\\x93\\x06\\n\\x90\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x02\\x00\\x01\\x00\\x10^\\x00\\x00\\x02\\x01\\x01\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x01\\x01\\x00\\x00\\x01\\x00e\\xc4\\xbb\\x08\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x01\\x01\\x00\\x01\\x00\\x00\\xa1\\xa7\\xa6p\\x0b\\x01\\x00\\x00\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x03\\x00y\\xd5\\xbb\\xb7\\xb4\\xb3\\x9b\\x93\\x81\\xaf\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x01\\x02\\x01\\x00w\\xc6\\xb7\\xb9\\xaa\\xb9\\xac\\xaa\\xaa\\x92\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x03\\x00\\x00\\xaf\\xd0\\xb0\\xd4\\xb4\\xae\\xa6\\xa4\\xa4\\x90\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00I\\xff\\xc0\\x86\\xaf\\xb7\\xc0\\xb8\\xbd\\xb3\\xc1\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x01\\x00\\x00F\\xc0\\xaa\\x86\\xbd\\xc0\\xaf\\x9d\\x9c\\xab\\x95\\xb4\\x08\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x01\\x03\\x01\\x00#c\\xb5\\xb7~\\xaf\\xc5\\xd0\\xcb\\xc5\\xbc\\xaf\\x9e\\xbb\\x0e\\x01\\x01\\x00\\x00\\x01\\x01\\x01\\x01\\x02\\x02\\x00\\x00F\\xbc\\xbct\\x83\\xb4\\xca\\xbe\\xb5\\xbc\\xbb\\xaf\\xa7\\x9c\\xc7\"\\x00\\x00\\x03\\x03\\x02\\x00\\x00\\x00\\x00\\x00\\x03E\\xb3j\\x86\\x97\\xbb\\xb7\\xb2\\xab\\xa9\\xbb\\xbb\\xb7\\xbc\\xa7\\xd25\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x0b\\x139r\\x82}\\x81\\xa0\\xae\\xb2\\xb8\\xb9\\xc4\\xc5\\xc6\\xc0\\xbc\\xbd\\xa6\\xd34\\x07\\x00\\x1dWXieclnn\\x89\\x9b\\xa6\\xae\\xb3\\xae\\xb0\\xb4\\xb5\\xb5\\xb4\\xb4\\xb8\\xae\\xa9\\xd3B\\x00\\x13\\x8fwstorwt}\\x8b\\x93\\x9b\\x9e\\xa1\\xaa\\xac\\xae\\xae\\xb3\\xbc\\xc0\\xb8\\xaa\\xae\\xcbU\\x00b\\xa2\\x94\\x92\\x8c\\x89\\x92\\x93\\x98\\x99\\x9b\\x9e\\xa4\\xa6\\xa9\\xab\\xac\\xb3\\xaf\\xb0\\xb4\\xbb\\xb4\\xb4\\xb7\\xc5\\\\1\\x80\\x85\\xa2\\xaf\\xb3\\xb2\\xa5\\xa2\\x9d\\x9e\\xa5\\xb2\\xb4\\xb4\\xbb\\xbe\\xc2\\xca\\xcf\\xd2\\xcd\\xd8\\xd9\\xd4\\xd4\\xd8^\\x1c\\x83\\x8a\\x8c\\x90\\xa1\\xab\\xb8\\xc4\\xc2\\xc2\\xc5\\xcd\\xd0\\xce\\xca\\xc9\\xc9\\xc5\\xc2\\xbe\\xb4\\xaf\\xa5\\x98\\x93\\x9dp\\x00\\x000t\\x9e\\xa4\\x97\\x9d\\xa0\\xa9\\xac\\xac\\xac\\xb7\\xb9\\xca\\xb5\\xab\\x98\\xaa\\xaa\\xa2\\xa7\\xaf\\xaa\\xa2\\x9d{\\x03\\x00\\x00\\x00\\x065i\\x8f\\xa9\\xa5\\xb9\\xb7\\xc2\\xacE&\\x14\\x01\\x00C\\xd8\\xd5\\xca\\xd2\\xd0\\xc6\\xc0\\x86\\x00\\x02\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x01\\x00\\x00\\x00\\x01\\x00\\x00/80)+\\'#\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\n\\x0e\\n\\x05width\\x12\\x05\\x1a\\x03\\n\\x01\\x1c'\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tmp = sess.run(next_element)\n",
    "    print(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need two things:\n",
    "1. A mechanism to handle the training set and the validation set differently.\n",
    "2. A way of introducing transformations into the serialized objects.\n",
    "\n",
    "For the former we can rely on either *(re)initializable* iterators or *feedable iterators*. These are illustrated in the [importing data](https://www.tensorflow.org/programmers_guide/datasets) guide. Let's consider an example with an initiable iterator. We create a placeholder that will store the string with the full path to the TFRecords object. We then create a node that create a TFRecord dataset using the information stored in this placeholder. Finally we create an initializable iterator that is initialized by the content of the placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n",
      "859\n"
     ]
    }
   ],
   "source": [
    "filenames = tf.placeholder(dtype=tf.string, shape=None)\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer, \n",
    "             feed_dict={filenames: '../data/fashion_mnist/val.tfrecords'})\n",
    "    next_train_example = sess.run(iterator.get_next())\n",
    "    print(type(next_train_example))\n",
    "    sess.run(iterator.initializer, \n",
    "             feed_dict={filenames: '../data/fashion_mnist/train.tfrecords'})\n",
    "    next_val_example = sess.run(iterator.get_next())\n",
    "    print(len(next_val_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying transformations to the elements of a dataset\n",
    "\n",
    "Our datasets contained the serialized string representation of the images and of their labels. We want to extract this information. Typical pipelines extract **tf.train.Example** protocol buffer *messages*. Each of these messages contains one or more *features*, and we typically want to convert these messages into tensors.\n",
    "\n",
    "Let's write a function that reads the protocol buffers messages from a TFRecordDataset and returns tensors. There are a few things to note:\n",
    "\n",
    "1. The input of `image_raw` are Numpy arrays, not images, therefore we use `tf.decode_raw`, not `tf.decode_image`.\n",
    "2. We need to convert `height` and `weight` into `tf.int32`, otherwise the reshaping operation fails.\n",
    "3. We reshape the labels (**TODO**: should have we done it beforehand?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_serialized_example_to_tensor(serialized_example):\n",
    "    \"\"\"Convert a serialized example into a normalized image.\"\"\"\n",
    "    features = {'image_raw': tf.FixedLenFeature((), tf.string, default_value=''),\n",
    "                'label': tf.FixedLenFeature((), tf.int64, default_value=0),\n",
    "                'height': tf.FixedLenFeature((), tf.int64, default_value=28),\n",
    "                'width': tf.FixedLenFeature((), tf.int64, default_value=28)}\n",
    "    parsed_features = tf.parse_single_example(serialized_example, features)\n",
    "    \n",
    "    label = tf.reshape(parsed_features['label'], [1]) # label is a tf.int64\n",
    "    \n",
    "    # Convert height and weight from tf.int64 to tf.int32\n",
    "    height, width = parsed_features['height'], parsed_features['width']\n",
    "    height, width = tf.cast(height, tf.int32), tf.cast(width, tf.int32)\n",
    "    \n",
    "    # Cast the decoded image into tf.uint8 and then into tf.float32\n",
    "    image = tf.cast(tf.decode_raw(parsed_features['image_raw'], tf.uint8), tf.float32)\n",
    "    image = tf.reshape(image / 255., [height, width])\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28) float32 (1,) int64\n"
     ]
    }
   ],
   "source": [
    "dataset = training_set.map(convert_serialized_example_to_tensor)\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    img, lbl = sess.run(next_element)\n",
    "    print(img.shape, img.dtype, lbl.shape, lbl.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching\n",
    "\n",
    "Once we have a deserialized dataset, generating batches is very simple, as it just require the use of the `batch` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 28, 28) (32, 1)\n"
     ]
    }
   ],
   "source": [
    "batched_dataset = dataset.batch(32)\n",
    "iterator = batched_dataset.make_one_shot_iterator()\n",
    "next_batch = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    image_batch, label_batch = sess.run(next_batch)\n",
    "    print(image_batch.shape, label_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
