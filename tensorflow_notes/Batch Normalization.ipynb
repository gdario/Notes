{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization in TensorFlow\n",
    "\n",
    "In this notebook we explore the proper way to use batch normalization in TensorFlow. TensorFlow has several functions related to batch normalization, in the `contrib`, `nn` and `layers` modules. The latter is supposed to be the more \"official\", and is the one that we will be using. We build a MLP with a (uselessly) large number of layers, all of the same size. The common wisdom is that batchnorm can, at least partially, replace dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from functools import partial\n",
    "\n",
    "mnist = input_data.read_data_sets('/tmp/data/')\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 100\n",
    "n_hidden3 = 100\n",
    "n_hidden4 = 100\n",
    "n_hidden5 = 100\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_epochs = 150\n",
    "batch_size = 100\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the usual placeholders for `X` and `y`, we need an extra one for the logical variable indicating whether we are in the training or in the test phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.placeholder(tf.int64, shape=(None), name='y')\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batchnorm layer should be between the inputs and the activation. We are going to share the parameters of the batch normalization layer across all the hidden layers, and we are going to use `functools.partial` for this. Similarly we are going to use the same initialization for all the hidden layers, and it makes sense to create a `my_dense_layer` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dense_layer = partial(\n",
    "    tf.layers.dense, \n",
    "    kernel_initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "\n",
    "my_batch_norm = partial(tf.layers.batch_normalization, \n",
    "                        training=training,\n",
    "                        momentum=0.9)\n",
    "\n",
    "with tf.name_scope('model'):\n",
    "    \n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name='hidden1')\n",
    "    bn1 = my_batch_norm(hidden1)\n",
    "    bn1_act = tf.nn.elu(bn1)\n",
    "    \n",
    "    hidden2 = my_dense_layer(X, n_hidden2, name='hidden2')\n",
    "    bn2 = my_batch_norm(hidden2)\n",
    "    bn2_act = tf.nn.elu(bn2)\n",
    "    \n",
    "    hidden3 = my_dense_layer(X, n_hidden3, name='hidden3')\n",
    "    bn3 = my_batch_norm(hidden3)\n",
    "    bn3_act = tf.nn.elu(bn3)\n",
    "    \n",
    "    hidden4 = my_dense_layer(X, n_hidden4, name='hidden4')\n",
    "    bn4 = my_batch_norm(hidden4)\n",
    "    bn4_act = tf.nn.elu(bn4)\n",
    "    \n",
    "    hidden5 = my_dense_layer(X, n_hidden5, name='hidden5')\n",
    "    bn5 = my_batch_norm(hidden5)\n",
    "    bn5_act = tf.nn.elu(bn5)\n",
    "    \n",
    "    logits_before_bn = tf.layers.dense(bn5_act, 10, name='output')\n",
    "    logits = my_batch_norm(logits_before_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now write the crossentropy and the loss, generate the training op, and compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=y, logits=logits, name='xentropy')\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [TensorFlow documentation](https://www.tensorflow.org/versions/master/api_docs/python/tf/layers/batch_normalization) recommends that *when training, the moving_mean and moving_variance need to be updated. By default the update ops are placed in tf.GraphKeys.UPDATE_OPS, so they need to be added as a dependency to the train_op*. In practical terms this means that we should use a `with tf.control_dependencies` term in the code containing the training op."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    # optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now launch a session and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 test accuracy: 0.9289\n",
      "10 test accuracy: 0.9752\n",
      "20 test accuracy: 0.9761\n",
      "30 test accuracy: 0.9765\n",
      "40 test accuracy: 0.9778\n",
      "50 test accuracy: 0.9733\n",
      "60 test accuracy: 0.9785\n",
      "70 test accuracy: 0.9779\n",
      "80 test accuracy: 0.9786\n",
      "90 test accuracy: 0.977\n",
      "100 test accuracy: 0.9788\n",
      "110 test accuracy: 0.9744\n",
      "120 test accuracy: 0.9783\n",
      "130 test accuracy: 0.9787\n",
      "140 test accuracy: 0.9768\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = sess.run(accuracy, feed_dict={X: mnist.test.images,\n",
    "                                                     y: mnist.test.labels})\n",
    "        if epoch % 10 == 0:\n",
    "            print(epoch, \"test accuracy:\", accuracy_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
