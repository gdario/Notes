{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression in TensorFlow: The California Housing Dataset\n",
    "\n",
    "In what follows we will use a small dataset to simulate a more realistic scenario. The California Housing dataset contains 20,640 entries and 8 predictors. We first pre-process the dataset by standardizing the predictors, and we save it into an HDF5 file. As a second step, we find the optimal values of the regression parameters using minibatch gradient descent reading from the HDF5 file. In the course of so doing we point out some of the possible pitfalls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "%matplotlib inline\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start extracting the data from the California Housing dataset and standardizing the features. We also add a column of \"biases\" that do not undergo the normalization step. Following the practices illustrated by A. Geron, we reshape the targets into a one-column array. We use the `np.c_` function to add a column of ones to the matrix. The syntax is a bit surprising, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "X_unsc = housing.data # Unscaled data\n",
    "y_ = housing.target.reshape(-1, 1) # 2D array instead of 1D\n",
    "ssc = StandardScaler()\n",
    "X_sc = ssc.fit_transform(X_unsc)\n",
    "X_sc = np.c_[np.ones(X_sc.shape[0]), X_sc] # Add the bias column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created an unscaled version of the data, a scaled version, and a scaled version with an additional column of ones, to represent the intercept or bias factor. All these objects, so far, are flot64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float64'), dtype('float64'), dtype('float64'))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_unsc.dtype, X_sc.dtype, y_.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a couple of constant, float32 objects, and use the normal equations to find the best fit, rather than using SGD. This shows how to create TensorFlow with a specific `dtype`, and exposes some linear algebra functions. We don't need to create a variable storing the value of the $\\theta$ parameters, as this will be obtained directly from the matrix operations, and namely from the normal equation: $$\\theta = (X^T X)^{-1} X^T y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.06856298]\n",
      " [ 0.82961965]\n",
      " [ 0.11875178]\n",
      " [-0.26552707]\n",
      " [ 0.30569667]\n",
      " [-0.00450281]\n",
      " [-0.03932635]\n",
      " [-0.8998825 ]\n",
      " [-0.87053877]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.constant(X_sc, dtype=tf.float32, name='X') # Store as float 32\n",
    "y = tf.constant(y_, dtype=tf.float32, name='y')    # Same\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    res = theta.eval()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above shows what we should be aiming for when trying to estimate the parameter $\\theta$ via gradient descent.\n",
    "\n",
    "## Fitting the linear model with SGD\n",
    "\n",
    "In this section we try to estimate the model parameters using Stochastic Gradient Descent. In the previous example $\\theta$ was directly calculated via the normal equation. In this case we define it as a variable and initialize it with random values from a uniform distribution. The predicted values are obtained as $\\hat y = X^T \\theta$ and the error is given by $e = y - \\hat y$. Note that `tf.square(error)` below will return an array, and to have a single number we need to summarize it, taking its mean. This is performed by `tf.reduce_mean()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "m, n = X_sc.shape\n",
    "\n",
    "X = tf.constant(X_sc, dtype=tf.float32, name='X') # Store as float 32\n",
    "y = tf.constant(y_, dtype=tf.float32, name='y')    # Same\n",
    "theta = tf.Variable(tf.random_uniform(shape=[n, 1], minval=-1, maxval=1), name='theta')\n",
    "y_pred = tf.matmul(X, theta, name='y_pred')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize the MSE, we make use of the `GradientDescentOptimizer` function from `tf.train`. We add a `training_op` node to the graph that calls the `optimizer` instance of the gradient descent optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.06855774e+00]\n",
      " [  8.75750244e-01]\n",
      " [  1.31303951e-01]\n",
      " [ -3.45384002e-01]\n",
      " [  3.68520677e-01]\n",
      " [ -5.35451574e-04]\n",
      " [ -4.13717031e-02]\n",
      " [ -7.65064240e-01]\n",
      " [ -7.40735531e-01]]\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "training_op = optimizer.minimize(mse)\n",
    "init = tf.global_variables_initializer() # We have a variable to initialize\n",
    "\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        sess.run(training_op)\n",
    "    res = theta.eval()\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we need to loop over the number of epochs we decide to train the model, and that within each epoch, we need to run the `training_op`, as this is the node that moves the parameter space towards the minimum. For this approach we have used constants to store `X` and `y`, as we haven't used mini-batch gradient descent. If we want to use it, however, we need to create some temporary storage for each batch, *i.e.*, we need *placeholders*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "N_BATCHES = int(np.ceil(m / BATCH_SIZE))\n",
    "\n",
    "X = tf.placeholder(shape=(None, n), dtype=tf.float32, name='X')\n",
    "y = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='y')\n",
    "theta = tf.Variable(tf.random_uniform(shape=[n, 1], minval=-1, maxval=1), name='theta')\n",
    "y_pred = tf.matmul(X, theta, name='y_pred')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function to produce the data batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_batch(X, y):\n",
    "    start = 0\n",
    "    for batch in range(N_BATCHES):\n",
    "        start = start + batch * BATCH_SIZE\n",
    "        end = start + BATCH_SIZE\n",
    "        X_batch = X[start:stop]\n",
    "        y_batch = y[start:stop]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch generator will return a batch of the specified size for each epoch. An improvement could be shuffling the samples during the generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 50\n",
      "Epoch: 100\n",
      "Epoch: 150\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'theta_eval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-56905142be5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mXb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mXb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mbest_theta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'theta_eval' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init) # Re-initialize theta\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        if epoch % 50 == 0:\n",
    "            print(\"Epoch: {}\".format(epoch))\n",
    "        for batch in range(N_BATCHES):\n",
    "            Xb, yb = next(batch_generator)\n",
    "            sess.run(training_op, feed_dict={X: Xb, y: yb})\n",
    "    best_theta = theta.val()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data from an HDF5 file\n",
    "\n",
    "We now create the HDF5 dataset that will store the data. Note that the data are of `dtype float64`. We don't convert them into `float32`, as this turns out not to be necessary. However, if our data were of much larger size, it could be a good idea to save a `float32` version to save memory. Note also that once we have created the file handler `hf` we can create datasets using the same name (`dset`, in our case). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to create file (Unable to truncate a file which is already open)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-dfe9a2a9e774>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'housing.hdf5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unsc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhousing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/home/ilan/minonda/conda-bld/h5py_1496889914775/work/h5py/_objects.c:2846)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/home/ilan/minonda/conda-bld/h5py_1496889914775/work/h5py/_objects.c:2804)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create (/home/ilan/minonda/conda-bld/h5py_1496889914775/work/h5py/h5f.c:2290)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to create file (Unable to truncate a file which is already open)"
     ]
    }
   ],
   "source": [
    "hf = h5py.File('housing.hdf5', 'w')\n",
    "dset = hf.create_dataset(name='X', data=X)\n",
    "dset = hf.create_dataset(name='y', data=y)\n",
    "hf.close()\n",
    "del X, X_sc, X_unsc, y, ssc, housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a linear regression model with minibatch gradient descent\n",
    "\n",
    "We reload the data, and fix some parameters for the learning phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File('housing.hdf5', 'r')\n",
    "X_tr = hf['X']\n",
    "y_tr = hf['y']\n",
    "X_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `X_tr` is an HDF5 dataset with 20640 rows and 9 columns. We can however already slice it and obtain the values without using the `[...]` selector. Note also that it is enough to specify a *slice*, and not both axis (like in `X_tr[:5, :]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m, n = X_tr.shape\n",
    "learning_rate = 0.01\n",
    "batch_size = 100\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a batch-generating function. Normally the function would return a generator. In this simplified case, we return the batches directly. In Geron's book the batches are created by producing random indices. Our approach here is a bit different, and takes directly the indices of the minibatches, that are computed inside the session. We slice inside the function, and we get, as argument, the start and end position of the slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(start, stop):\n",
    "    return X_tr[slice(start, stop)], y_tr[slice(start, stop)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test whether the function works using a small index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_x, tmp_y = get_batch(start=0, stop=5)\n",
    "tmp_x.shape, tmp_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction of the graph\n",
    "\n",
    "Our graph is very simple. We have two placeholders for the `X` and the `y` and a variable that stores the values of `theta` (9 components). We initialize the variable with uniform values between -1 and 1. The predicted values are simply obtained by multiplying `X` by `theta`. We use a Gradient Descent Optimizer and we use it to minimize the Mean Squared Error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=(None, n), name='X')\n",
    "    y = tf.placeholder(dtype=tf.float32, shape=(None, 1), name='y')\n",
    "    theta = tf.Variable(tf.random_uniform(\n",
    "        [n, 1], -1.0, 1.0, seed=42), name='theta')\n",
    "    y_pred = tf.matmul(X, theta, name='y_pred')\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(mse)\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution phase\n",
    "\n",
    "For the execution phase we compute the number of batches from the `batch_size` and the total number of observations. We generate a start and end position for each minibatch and generate the minibatches with the `get_batch` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch: {}\".format(epoch))\n",
    "        num_batches = m // batch_size\n",
    "        for batch_index in range(num_batches):\n",
    "            start = batch_size * batch_index\n",
    "            stop = batch_size * (batch_index + 1)\n",
    "            X_batch, y_batch = get_batch(start, stop)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "    best_theta = theta.eval()\n",
    "print(best_theta)\n",
    "hf.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
